{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nall function of this notebook(and some other code.) is based on fallowing amazing github repo:\\n\\nhttps://github.com/janvanrijn/openml-pimp\\n\\nSo all credits goes for it.\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# imports\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import median\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "%matplotlib\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    " \n",
    " \n",
    "font = {\n",
    "        'family' : 'serif',\n",
    "        'size'   : 26}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "# Rebuild the matplotlib font cache\n",
    "fm._rebuild()\n",
    "\"\"\"\n",
    "all function of this notebook(and some other code.) is based on fallowing amazing github repo:\n",
    "\n",
    "https://github.com/janvanrijn/openml-pimp\n",
    "\n",
    "So all credits goes for it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_dict(dictionary, reverse=False):\n",
    "    '''\n",
    "    Get a dictionary and return a rank dictionary\n",
    "    for example dic={'a':10,'b':2,'c':6}\n",
    "    will return dic={'a':1.0,'b':3.0,'c':2.0}\n",
    "    \n",
    "    '''\n",
    "    dictionary = copy.copy(dictionary)\n",
    "    \n",
    "    if reverse:\n",
    "        \n",
    "        for key in dictionary.keys():\n",
    "            dictionary[key] = 1 - dictionary[key]\n",
    "                      \n",
    "    sortdict = collections.OrderedDict(sorted(dictionary.items()))\n",
    "    ranks = scipy.stats.rankdata(list(sortdict.values()))\n",
    "    result = {}\n",
    "    \n",
    "    for idx, (key, value) in enumerate(sortdict.items()):\n",
    "        result[key] = ranks[idx]\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_dict_values(a, b, allow_subsets=False):\n",
    "    '''\n",
    "    Get two dictionary sum them together!\n",
    "    '''\n",
    "    result = {}\n",
    "    a_total = sum(a.values())\n",
    "    b_total = sum(b.values())\n",
    "    a_min_b = set(a.keys()) - set(b.keys())\n",
    "    b_min_a = set(b.keys()) - set(a.keys())\n",
    "    \n",
    "#     if len(b_min_a) > 0:\n",
    "#         raise ValueError('dict b got illegal keys: %s' %str(b_min_a))\n",
    "        \n",
    "#     if not allow_subsets and len(a_min_b):\n",
    "#         raise ValueError('keys not the same')\n",
    "        \n",
    "    for idx in a.keys():\n",
    "        if idx in b:\n",
    "            result[idx] = a[idx] + b[idx]\n",
    "        else:\n",
    "            result[idx] = a[idx]\n",
    "            \n",
    "#     if sum(result.values()) != a_total + b_total:\n",
    "#         raise ValueError()\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_dict_values(d, denominator):\n",
    "    ''' \n",
    "    divide d/demoniator\n",
    "    '''\n",
    "    result = {}\n",
    "    \n",
    "    for idx in d.keys():\n",
    "        result[idx] = d[idx] / denominator\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_relevant(data, max_items=None, max_interactions=None):\n",
    "\n",
    "\n",
    "\n",
    "    sorted_values = []\n",
    "    keys = []\n",
    "    interactions_seen = 0\n",
    "\n",
    "\n",
    "    for key in sorted(data, key=lambda k: median(data[k]), reverse=True):\n",
    "        if '__' in key:\n",
    "            interactions_seen += 1\n",
    "            if interactions_seen > max_interactions:\n",
    "                continue\n",
    "\n",
    "        sorted_values.append(data[key])\n",
    "        keys.append(key)\n",
    "\n",
    "\n",
    "    if max_items is not None:\n",
    "        sorted_values = sorted_values[:max_items]\n",
    "        keys = keys[:max_items]\n",
    "\n",
    "    return sorted_values, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_plots(sorted_values, keys, fig_title):\n",
    "    sorted_values=sorted_values[0:9]\n",
    "    plt.figure(figsize=(12,10))\n",
    "    lst_arr=[]\n",
    "    lst_arr.append(list(sorted_values))\n",
    "    lst_arr.append(list(range(len(sorted_values))))\n",
    "    \n",
    "    sns.violinplot(data=list(sorted_values),inner='box', scale='width', cut=0, linewidth=2  )\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    keys = [format_name(key) for key in keys]\n",
    "    # plt.set_xticklabels(list(range(len(sorted_values))))\n",
    "    plt.xticks(list(range(len(sorted_values))), list(keys), rotation=45, ha='right')\n",
    "    plt.ylabel('Variance Contribution')\n",
    "    sns.set_palette(\"RdBu\")\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    ax=plt.gcf()\n",
    " \n",
    " \n",
    "    # plt.show()\n",
    "    ax.savefig(\"../output_plots/\"+fig_title+\".pdf\" ,bbox_inches = 'tight',pad_inches = 0, format='pdf')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_name(name):\n",
    "    '''\n",
    "    Format hyperparameter names!\n",
    "    '''\n",
    "    mapping_short = {\n",
    "        'strategy': 'imputation',\n",
    "        'max_features': 'max. features',\n",
    "        'min_samples_leaf': 'min. samples leaf',\n",
    "        'min_samples_split': 'min. samples split',\n",
    "        'criterion': 'criterion',\n",
    "        'learning_rate': 'learning rate',\n",
    "        'max_depth': 'max. depth',\n",
    "        'n_estimators': 'n. estimators',\n",
    "        'algorithm': 'algorithm',\n",
    "        \"('max_features', 'min_samples_leaf')\":\"max. feat., min. s. leaf\",\n",
    "        \"('criterion', 'max_features', 'min_samples_leaf')\":\"criterion, max. feat., min. s. leaf\",\n",
    "        \"('max_features', 'min_samples_split')\":\"criterion, max. feat., min. s. split\",\n",
    "        \"('bootstrap', 'max_features', 'min_samples_leaf')\": \"boots., max. feat., min. s. leaf\",\n",
    "        \"('bootstrap', 'max_features')\": \"boots., max. feat.\",\n",
    "        \"('bootstrap', 'min_samples_leaf')\":\"boots., min. s. leaf\",\n",
    "        \"('imputation', 'max_features', 'min_samples_leaf')\": \"imput., max. feat., min. s. leaf\",\n",
    "        \"('imputation', 'max_features')\": \"imput., max. feat.\",\n",
    "        \"('imputation', 'min_samples_leaf')\": \"imput., min. s. leaf\",\n",
    "        \"('min_samples_leaf', 'min_samples_split')\":\"min. s. leaf, min. s. split\",\n",
    "        \"('min_samples_leaf', 'n_estimators')\":\"min. s. leaf, n. esti.\",\n",
    "        \"('learning_rate', 'max_depth')\": \"learning r., max. depth\",\n",
    "        \"('learning_rate', 'max_features')\": \"learning r., max. feat.\",\n",
    "        \"('max_depth', 'n_estimators')\":\"max. depth, n. esti.\",\n",
    "        \"('learning_rate', 'n_estimators')\": \"learning r., n esti.\",\n",
    "        \"('learning_rate', 'min_samples_leaf')\": \"learning r., min. s.leaf\",\n",
    "        \"('algorithm', 'max_depth', 'learning_rate')\":\"alg., max. depth, learning r.\",\n",
    "        \"('algorithm', 'max_depth')\": \"alg., max_depth\",\n",
    "        \"('imputation', 'max_depth', 'learning_rate')\":\"imput., max. depth, learning r.\",\n",
    "        \"('gamma', 'kernel')\":\"gamma, kernel\",\n",
    "        \"('imputation', 'kernel')\":\"imput., kernel\",\n",
    "        \"('imputation', 'tol')\":\"imput., tol\",\n",
    "        \"('C', 'imputation')\":\"C, imput.\",\n",
    "        \"('coef0', 'gamma')\":\"coef0, gamma\",\n",
    "        \"('coef0', 'imputation')\":\"coef0, imput.\",\n",
    "        \"('gamma', 'imputation')\":\"gamma, imput.\",\n",
    "\n",
    "\n",
    "\n",
    "        }\n",
    "\n",
    "    parts = name.split('__')\n",
    "    for idx, part in enumerate(parts):\n",
    "        if part in mapping_short:\n",
    "            parts[idx] = mapping_short[part]\n",
    " \n",
    "                \n",
    "    return ' / '.join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df=df.drop([\"imputation\",\"time_taken\"],axis=1)\n",
    "def obtain_marginal_contributions(df):\n",
    "    '''\n",
    "    This is the main function that calls Top functions\n",
    "    '''\n",
    "    \n",
    "    all_ranks = dict()\n",
    "    all_tasks = list()\n",
    "    total_ranks = None\n",
    "    num_tasks = 0\n",
    "    marginal_contribution = collections.defaultdict(list)\n",
    "\n",
    "    lst_datasets=list(df.dataset.unique())\n",
    "\n",
    "    for dataset in lst_datasets:\n",
    "\n",
    "\n",
    "        a=df[df.dataset==dataset]\n",
    "        a=a.drop(\"dataset\",axis=1)\n",
    "        param=dict()\n",
    "\n",
    "\n",
    "        for index, row in a.iterrows():\n",
    "            marginal_contribution[row[\"param\"]].append(row[\"importance\"])\n",
    "            param.update( {row[\"param\"] : row[\"importance\"]} )\n",
    "\n",
    "        ranks = rank_dict(param, reverse=True)\n",
    "        if total_ranks is None:\n",
    "            total_ranks = ranks\n",
    "        else:\n",
    "            total_ranks = sum_dict_values( ranks,total_ranks, allow_subsets=False)\n",
    "            num_tasks += 1\n",
    "    total_ranks = divide_dict_values(total_ranks, num_tasks)\n",
    "    return total_ranks, marginal_contribution, lst_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Result: 6 plot over 200 datasets for 6 classifiers(obtain_marginal_contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\"RF\",\"SVM\",\"ET\",\"DT\",\"AB\",\"GB\"}\n",
    "for cls in {\"RF\",\"SVM\",\"ET\",\"DT\",\"AB\",\"GB\"}:\n",
    "    df=pd.read_csv(\"../PerformanceData/\"+cls+\"_fANOVA_results.csv\")\n",
    "    total_ranks, marginal_contribution, _ = obtain_marginal_contributions(df)\n",
    "    sorted_values, keys = determine_relevant(marginal_contribution, max_interactions=3)\n",
    "    marginal_plots(sorted_values, keys, cls+\" classifier\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "012080076d34b6fd00b64d41aae62f55212ca98f0a386ccc71b4094bc25b8854"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}