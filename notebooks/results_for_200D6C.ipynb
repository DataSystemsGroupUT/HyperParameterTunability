{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using matplotlib backend: Qt5Agg\n"
    }
   ],
   "source": [
    "# imports\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import median\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "%matplotlib\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_dict(dictionary, reverse=False):\n",
    "    '''\n",
    "    Get a dictionary and return a rank dictionary\n",
    "    for example dic={'a':10,'b':2,'c':6}\n",
    "    will return dic={'a':1.0,'b':3.0,'c':2.0}\n",
    "    \n",
    "    '''\n",
    "    dictionary = copy.copy(dictionary)\n",
    "    \n",
    "    if reverse:\n",
    "        \n",
    "        for key in dictionary.keys():\n",
    "            dictionary[key] = 1 - dictionary[key]\n",
    "                      \n",
    "    sortdict = collections.OrderedDict(sorted(dictionary.items()))\n",
    "    ranks = scipy.stats.rankdata(list(sortdict.values()))\n",
    "    result = {}\n",
    "    \n",
    "    for idx, (key, value) in enumerate(sortdict.items()):\n",
    "        result[key] = ranks[idx]\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_dict_values(a, b, allow_subsets=False):\n",
    "    '''\n",
    "    Get two dictionary sum them together!\n",
    "    '''\n",
    "    result = {}\n",
    "    a_total = sum(a.values())\n",
    "    b_total = sum(b.values())\n",
    "    a_min_b = set(a.keys()) - set(b.keys())\n",
    "    b_min_a = set(b.keys()) - set(a.keys())\n",
    "    \n",
    "#     if len(b_min_a) > 0:\n",
    "#         raise ValueError('dict b got illegal keys: %s' %str(b_min_a))\n",
    "        \n",
    "#     if not allow_subsets and len(a_min_b):\n",
    "#         raise ValueError('keys not the same')\n",
    "        \n",
    "    for idx in a.keys():\n",
    "        if idx in b:\n",
    "            result[idx] = a[idx] + b[idx]\n",
    "        else:\n",
    "            result[idx] = a[idx]\n",
    "            \n",
    "#     if sum(result.values()) != a_total + b_total:\n",
    "#         raise ValueError()\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_dict_values(d, denominator):\n",
    "    ''' \n",
    "    divide d/demoniator\n",
    "    '''\n",
    "    result = {}\n",
    "    \n",
    "    for idx in d.keys():\n",
    "        result[idx] = d[idx] / denominator\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_relevant(data, max_items=None, max_interactions=None):\n",
    "\n",
    "\n",
    "\n",
    "    sorted_values = []\n",
    "    keys = []\n",
    "    interactions_seen = 0\n",
    "\n",
    "\n",
    "    for key in sorted(data, key=lambda k: median(data[k]), reverse=True):\n",
    "        if '__' in key:\n",
    "            interactions_seen += 1\n",
    "            if interactions_seen > max_interactions:\n",
    "                continue\n",
    "\n",
    "        sorted_values.append(data[key])\n",
    "        keys.append(key)\n",
    "\n",
    "\n",
    "    if max_items is not None:\n",
    "        sorted_values = sorted_values[:max_items]\n",
    "        keys = keys[:max_items]\n",
    "\n",
    "    return sorted_values, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_plots(sorted_values, keys, fig_title):\n",
    "    \n",
    "    sorted_values=sorted_values[0:7]\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.violinplot(list(sorted_values), list(range(len(sorted_values))))\n",
    "    plt.plot([-0.5, len(sorted_values) - 0.5], [0, 0], 'k-', linestyle='--', lw=1)\n",
    "    keys = [format_name(key) for key in keys]\n",
    "    plt.xticks(list(range(len(sorted_values))), list(keys), rotation=45, ha='right')\n",
    "    plt.ylabel('marginal contribution')\n",
    "    plt.xlabel(\"Hyperparameters\")\n",
    "    plt.title(fig_title)\n",
    "    plt.show()\n",
    "    plt.savefig(\"../output_plots/\"+fig_title+\".jpg\" ,bbox_inches = 'tight',pad_inches = 0)\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_name(name):\n",
    "    '''\n",
    "    Format hyperparameter names!\n",
    "    '''\n",
    "    mapping_plain = {\n",
    "        'strategy': 'imputation',\n",
    "        'max_features': 'max. features',\n",
    "        'min_samples_leaf': 'min. samples leaf',\n",
    "        'min_samples_split': 'min. samples split',\n",
    "        'criterion': 'split criterion',\n",
    "        'learning_rate': 'learning rate',\n",
    "        'max_depth': 'max. depth',\n",
    "        'n_estimators': 'iterations',\n",
    "        'algorithm': 'algorithm',\n",
    "    }\n",
    "    \n",
    "    mapping_short = {\n",
    "        'strategy': 'imputation',\n",
    "        'max_features': 'max. feat.',\n",
    "        'min_samples_leaf': 'samples leaf',\n",
    "        'min_samples_split': 'samples split',\n",
    "        'criterion': 'split criterion',\n",
    "        'learning_rate': 'learning r.',\n",
    "        'max_depth': 'max. depth',\n",
    "        'n_estimators': 'iterations',\n",
    "        'algorithm': 'algo.',\n",
    "    }\n",
    "\n",
    "    parts = name.split('__')\n",
    "    \n",
    "    for idx, part in enumerate(parts):\n",
    "        if part in mapping_plain:\n",
    "            if len(parts) < 3:\n",
    "                parts[idx] = mapping_plain[part]\n",
    "            else:\n",
    "                parts[idx] = mapping_short[part]\n",
    "\n",
    "                \n",
    "    return ' / '.join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df=df.drop([\"imputation\",\"time_taken\"],axis=1)\n",
    "\n",
    "\n",
    "def obtain_marginal_contributions(df):\n",
    "    '''\n",
    "    This is the main function that calls Top functions\n",
    "    '''\n",
    "    \n",
    "    all_ranks = dict()\n",
    "    all_tasks = list()\n",
    "    total_ranks = None\n",
    "    num_tasks = 0\n",
    "    marginal_contribution = collections.defaultdict(list)\n",
    "\n",
    "    lst_datasets=list(df.dataset.unique())\n",
    "\n",
    "    for dataset in lst_datasets:\n",
    "\n",
    "\n",
    "        a=df[df.dataset==dataset]\n",
    "        a=a.drop(\"dataset\",axis=1)\n",
    "        param=dict()\n",
    "\n",
    "\n",
    "        for index, row in a.iterrows():\n",
    "            marginal_contribution[row[\"param\"]].append(row[\"importance\"])\n",
    "            param.update( {row[\"param\"] : row[\"importance\"]} )\n",
    "\n",
    "        ranks = rank_dict(param, reverse=True)\n",
    "        if total_ranks is None:\n",
    "            total_ranks = ranks\n",
    "        else:\n",
    "            total_ranks = sum_dict_values( ranks,total_ranks, allow_subsets=False)\n",
    "            num_tasks += 1\n",
    "    total_ranks = divide_dict_values(total_ranks, num_tasks)\n",
    "    return total_ranks, marginal_contribution, lst_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Result: 6 plot over 200 datasets for 6 classifiers(obtain_marginal_contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\"RF\",\"SVM\",\"ET\",\"DT\",\"AB\",\"GB\"}\n",
    "for cls in {\"RF\",\"SVM\",\"ET\",\"DT\",\"AB\",\"GB\"}:\n",
    "    df=pd.read_csv(\"../PerformanceData/\"+cls+\"_fANOVA_results.csv\")\n",
    "    total_ranks, marginal_contribution, _ = obtain_marginal_contributions(df)\n",
    "    sorted_values, keys = determine_relevant(marginal_contribution, max_interactions=3)\n",
    "    marginal_plots(sorted_values, keys, cls+\" classifier\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}